{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (week4) Home Credit Default Risk\n",
    "\n",
    "![img](https://storage.googleapis.com/kaggle-competitions/kaggle/9120/logos/header.png)\n",
    "\n",
    ">`Cf.`\n",
    "> + [Home-Credit-Default-Risk - github](https://github.com/rishabhrao1997/Home-Credit-Default-Risk/blob/main/EDA%20-%20Home%20Credit%20Default.ipynb)\n",
    "> + [HOME CREDIT DEFAULT RISK — An End to End ML Case Study — PART 1: Introduction and EDA - medium](https://medium.com/thecyphy/home-credit-default-risk-part-1-3bfe3c7ddd7a)\n",
    "> + [HOME CREDIT DEFAULT RISK — An End to End ML Case Study — PART 1: Introduction and EDA](https://medium.com/thecyphy/home-credit-default-risk-part-1-3bfe3c7ddd7a)\n",
    "> + [HOME CREDIT DEFAULT RISK — An End to End ML Case Study — PART 2: Feature Engineering and Modelling](https://medium.com/thecyphy/home-credit-default-risk-part-2-84b58c1ab9d5)\n",
    "> + [機械学習によく使うPythonのコード一覧まとめ - AI研究所](https://ai-kenkyujo.com/2020/06/08/kikaigakusyu-python/#i)\n",
    "\n",
    "> ```point.```\n",
    "> Supervised Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier #XGBoost\n",
    "from lightgbm import LGBMClassifier #LightGBM\n",
    "\n",
    "# evaluations\n",
    "from sklearn.metrics import accuracy_score # 正解率\n",
    "from sklearn.metrics import precision_score # 適合率\n",
    "from sklearn.metrics import recall_score # 再現率\n",
    "from sklearn.metrics import f1_score # F値\n",
    "from sklearn.metrics import confusion_matrix # 混合行列\n",
    "\n",
    "# visualization\n",
    "import missingno as msn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import IPython\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題1】コンペティション内容の確認\n",
    "> + (a) 何を学習し、何を予測するのか\n",
    "> + (b) どのようなファイルを作りKaggleに提出するか\n",
    "> + (c) 提出されたものはどういった指標値で評価されるのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a): Targetになりうるクライアントの返済能力を予測\n",
    "\n",
    "(b): [提出ファイル - kaggle](https://www.kaggle.com/c/home-credit-default-risk/overview/evaluation)で指定されているファイルを作成して提出\n",
    "\n",
    "(c): 予測された確率と観察されたターゲットの間の[ROC曲線](https://ja.wikipedia.org/wiki/%E5%8F%97%E4%BF%A1%E8%80%85%E6%93%8D%E4%BD%9C%E7%89%B9%E6%80%A7)の下の領域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題2】学習と検証\n",
    "> データを簡単に分析、前処理し、学習、検証するまでの一連の流れを作成・実行してください。\n",
    "\n",
    "> `memo`\n",
    "> Kagglerは最低限のデータセットでSubmitしてみて、Scoreがどの程度かを検証したりするそう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('input/application_train.csv')\n",
    "test_raw = pd.read_csv('input/application_test.csv')\n",
    "print('The size of the train data :', train_raw.shape)\n",
    "print('The size of the test data :', test_raw.shape)\n",
    "\n",
    "#train testのフラグつける\n",
    "train_mid = train_raw.copy()\n",
    "train_mid['train_or_test'] = 'train'\n",
    "test_mid = test_raw.copy()\n",
    "test_mid['train_or_test'] = 'test'\n",
    "test_mid['TARGET'] = 0.5\n",
    "\n",
    "alldata = pd.concat([train_mid, test_mid], sort=False, axis=0).reset_index(drop=True)\n",
    "print('The size of the alldata data:', alldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### （問題３まで、NaN全部消しバージョン）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - EDA / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損データの分布確認\n",
    "msn.matrix(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損データall除去\n",
    "alldata = alldata.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alldata.shape)\n",
    "print(alldata.isnull().sum())\n",
    "msn.matrix(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オブジェクト型を全て Label-Encoding\n",
    "alldata.columns[alldata.dtypes == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.get_dummies(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.columns[alldata.dtypes == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初に統合したtrainとtestを分離\n",
    "train_feature = alldata[alldata['train_or_test_train']==1]\n",
    "test_feature = alldata[alldata['train_or_test_train']==0]\n",
    "train_target = train_feature[\"TARGET\"]\n",
    "print(\"train: {}\".format(train_feature.shape))\n",
    "print(\"test: {}\".format(test_feature.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feature, train_target, test_size=0.2, random_state=0)\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = y_train.sum() / len(y_train)\n",
    "print(f'Target rate:{ratio}')\n",
    "print(f'base line accuracy: {1 - ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化\n",
    "sts = StandardScaler()\n",
    "sts.fit(X_train, y_train)\n",
    "X_train_norm = sts.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Machine Learning\n",
    "    ロジスティック回帰、ランダムフォレストで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "logreg_ev = evaluations(y_test, y_pred, \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "rfc_ev = evaluations(y_test, y_pred, \"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Evaliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluations(test, predict, average):\n",
    "    accuracy = accuracy_score(test, predict)\n",
    "    precision = precision_score(test, predict, average=average)\n",
    "    recall = recall_score(test, predict, average=average)\n",
    "    f1 = f1_score(test, predict, average=average)\n",
    "    evaluations = {\n",
    "        \"正解率\" : round(accuracy, 3), \n",
    "        \"適合率\" : round(precision, 3),\n",
    "        \"再現率\" : round(recall, 3), \n",
    "        \"F値\" : round(f1, 3)\n",
    "    }\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([logreg_ev, rfc_ev], index=[\"ロジスティック回帰\", \"ランダムフォレスト\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しにRondomforestの`feature_importances_`を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "# n = rfc.feature_importances_\n",
    "n = np.argsort(rfc.feature_importances_) # 数列の順位的なの返す\n",
    "x = X_train.columns[n]\n",
    "y = rfc.feature_importances_[n]\n",
    "\n",
    "plt.figure(figsize=(20, 50))\n",
    "plt.barh(x, y, label=\"Rondom Forest Classfire\")\n",
    "plt.title('RandomForestClassifier feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題3】テストデータに対する推定\n",
    "> テストデータ（`application_test.csv`）に対して推定を行い、Kaggleに提出を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_feature.values # (1739, 238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "logreg.fit(X_train, y_train) #(6881, 238) (6881, )\n",
    "logreg_pred = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([logreg_pred[:,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame([test_feature.values[:,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([b, a], axis=1)\n",
    "submit = submit.rename(columns={0 : \"SK_ID_CURR\", 1 : \"TARGET\" })\n",
    "submit.to_csv('output/demo_logreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題4】特徴量エンジニアリング\n",
    ">     精度を上げるために以下のような観点で 特徴量エンジニアリング（Feature Engineering） を行ってください。\n",
    ">        - どの特徴量を使うか\n",
    ">        - どう前処理をするか\n",
    ">     何をした時に検証データに対する評価指標がどのようになったかをまとめてください。最低5パターンの学習・検証を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_mid.head(3))\n",
    "display(test_mid.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_mid, test_mid]).reset_index(drop=True)\n",
    "display(df.head(3))\n",
    "display(df.tail(3))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - EDA2 / Preprocessing2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN's countup\n",
    "nan = df.isnull().sum().reset_index()\n",
    "nan.columns = [\"name\", \"count\"]\n",
    "nan[\"ratio\"] = (nan[\"count\"] / df.shape[0])*100\n",
    "nan[\"usabilty\"] = np.where(nan[\"ratio\"] > 20, \"Discard\", \"Keep\")\n",
    "nan = nan[nan[\"count\"] > 0].sort_values(by=\"ratio\")\n",
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN's plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=nan[\"name\"], y=nan[\"ratio\"])\n",
    "plt.xticks(rotation=90) #90°傾け\n",
    "plt.title(\"Feature containing NaN.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan Discardのカラム名をdfから除去\n",
    "drop_list = nan[nan[\"usabilty\"] == \"Discard\"][\"name\"].values.tolist()\n",
    "df = df.drop(drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# columns: \", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# こいつらをどうしようか…\n",
    "keep_nan = nan[nan[\"usabilty\"]==\"Keep\"]\n",
    "keep_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objリストのカラムを数値化\n",
    "\n",
    "# リスト作成\n",
    "obj_list = df.columns[df.dtypes == \"object\"].tolist()\n",
    "print(obj_list, len(obj_list), type(obj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変換したdf作成\n",
    "obj_df = pd.get_dummies(df[obj_list])\n",
    "display(obj_df.head(3))\n",
    "print(len(obj_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_listのカラム名の列を削除\n",
    "df = df.drop(obj_list, axis=1)\n",
    "# obj_dfとデータを結合\n",
    "df = pd.concat([df, obj_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.dtypes==\"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "f, ax = plt.subplots(figsize=(30, 25))\n",
    "mat = df.corr(\"pearson\")\n",
    "mask = np.triu(np.ones_like(mat, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[\"TARGET\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = mat.columns[mat[\"TARGET\"] < 0].tolist()\n",
    "drop_list.remove(\"SK_ID_CURR\")\n",
    "drop_list.remove(\"train_or_test_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Cf.`\n",
    "> + [2019 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2019/discussion/122021)\n",
    "> + [初めてのLightGBM](https://fukki.pythonanywhere.com/post_detail/19/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初に統合したtrainとtestを分離\n",
    "train = df[df['train_or_test_train'] == 1]\n",
    "test = df[df['train_or_test_test'] == 1]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲット変数と、学習に不要なカラムを定義\n",
    "target_col = \"TARGET\"\n",
    "drop_col = drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に必要な特徴量のみを保持\n",
    "train_feature = train.drop(drop_col, axis=1)\n",
    "test_feature = test.drop(drop_col, axis=1)\n",
    "train_tagert = train[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list2 = train_feature.columns[train_feature.isnull().sum() >0 ].tolist()\n",
    "drop_list3 = test_feature.columns[test_feature.isnull().sum() >0 ].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = train.drop(drop_list2, axis=1)\n",
    "test_feature = test.drop(drop_list3, axis=1)\n",
    "\n",
    "# LightGBMではJSON形式はcolumn名に使われているとparseができずにエラーが起こる\n",
    "# train_feature.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータを分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_feature, train_tagert, test_size=0.2, random_state=0, stratify=train_tagert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Baseline\n",
    "> (精度の基準となるモデル)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainから頻度に応じて単純なモデルを作る場合\n",
    "survive_rate = y_train.sum()/len(y_train)\n",
    "print(f'survive rate:{survive_rate}')\n",
    "print(f'base line accuracy: {1 - survive_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化\n",
    "sts = StandardScaler()\n",
    "sts.fit(X_train, y_train)\n",
    "X_train_norm = sts.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Machine Leaning\n",
    "> `Cf.`\n",
    "> + [.scoreで出てくる決定係数の解釈 - teratail](https://teratail.com/questions/100203)\n",
    "> + [機械学習ライブラリ scikit-learnの便利機能の紹介 - Qiita](https://qiita.com/ishizakiiii/items/0650723cc2b4eef2c1cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"=\"*20)\n",
    "print(\"LogisticRegression\")\n",
    "print(\"train acc: \", logreg.score(X_train, y_train))\n",
    "print(\"test acc: \", logreg.score(X_test, y_test))\n",
    "\n",
    "# # RandomForest\n",
    "# rfc = RandomForestClassifier()\n",
    "# rfc.fit(train_feature, train_tagert)\n",
    "# print(\"=\"*20)\n",
    "# print(\"RandomForest\")\n",
    "# print(\"train acc: \", rfc.score(X_train, y_train))\n",
    "# print(\"test acc: \", rfc.score(X_test, y_test))\n",
    "\n",
    "# # XGBoost\n",
    "# xgb = XGBClassifier()\n",
    "# xgb.fit(train_feature, train_tagert)\n",
    "# print(\"=\"*20)\n",
    "# print(\"XGBBoost\")\n",
    "# print(\"train acc: \", xgb.score(X_train, y_train))\n",
    "# print(\"test acc: \", xgb.score(X_test, y_test))\n",
    "\n",
    "# # LightGBM\n",
    "# lgb = LGBMClassifier()\n",
    "# lgb.fit(train_feature, train_tagert)\n",
    "# print(\"=\"*20)\n",
    "# print(\"LightGBM\")\n",
    "# print(\"train acc: \", lgb.score(X_train, y_train))\n",
    "# print(\"test acc: \", lgb.score(X_test, y_test))\n",
    "\n",
    "# # SVC\n",
    "# svc = SVC()\n",
    "# svc.fit(train_feature, train_tagert)\n",
    "# print(\"=\"*20)\n",
    "# print(\"SVC\")\n",
    "# print(\"train acc: \", svc.score(X_train, y_train))\n",
    "# print(\"test acc: \", svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_feature, train_tagert)\n",
    "\n",
    "# RandomForest\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(train_feature, train_tagert)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(train_feature, train_tagert)\n",
    "\n",
    "# LightGBM\n",
    "lgb = LGBMClassifier()\n",
    "lgb.fit(train_feature, train_tagert)\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc.fit(train_feature, train_tagert)\n",
    "\n",
    "# 推論\n",
    "pred = {\n",
    "    'rfc': rfc.predict(test_feature),\n",
    "    'xgb': xgb.predict(test_feature),\n",
    "    'lgb': lgb.predict(test_feature),\n",
    "    'logreg': logreg.predict(test_feature),\n",
    "    'svc': svc.predict(test_feature)\n",
    "}\n",
    "\n",
    "# ファイル出力\n",
    "for key, value in pred.items():\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(test.PassengerId, columns=[\"SK_ID_CURR\"]).reset_index(drop=True),\n",
    "            pd.DataFrame(value, columns=[\"TARGET\"])\n",
    "        ],\n",
    "        axis=1\n",
    "    ).to_csv(f'output/{key}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
