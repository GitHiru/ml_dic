{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (week5)Sprint機械学習フロー\n",
    "> `aim.`\n",
    "> + 機械学習の実践的な流れを知る\n",
    "> + 高い汎化性能を持つモデルを完成させる\n",
    "\n",
    ">     Kaggleの Home Credit Default Risk コンペティションを題材に、機械学習の実践的な流れを学びます。\n",
    ">     特に適切な 検証 を行い、高い 汎化性能 のあるモデルを完成させることを目指します。\n",
    "\n",
    "> `Cf.`\n",
    "> + [Home Credit Default Risk | Kaggle](https://www.kaggle.com/c/home-credit-default-risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Algorithm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Validation/Tuning\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "iris_features = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "iris_labels = pd.Series(iris.target)\n",
    "\n",
    "display(iris_features.head(3))\n",
    "display(iris_labels.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_train = train_test_split(\n",
    "    iris_features, iris_labels, test_size=0.5, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t/.pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:06:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 0, 0, 2, 1, 0, 2, 0,\n",
       "       1, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 2,\n",
       "       0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 1, 1,\n",
       "       0, 0, 0, 0, 2, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題1】クロスバリデーション\n",
    ">      事前学習期間では検証データをはじめに分割しておき、それに対して指標値を計算することで検証を行っていました。（ホールドアウト法）\n",
    ">      しかし、分割の仕方により精度は変化します。実践的には クロスバリデーション（交差検証） を行います。\n",
    ">      分割を複数回行い、それぞれに対して学習と検証を行う方法です。\n",
    ">      複数回の分割のためにscikit-learnにはKFoldクラスが用意されています。\n",
    ">      事前学習期間の課題で作成したベースラインモデルに対してKFoldクラスによるクロスバリデーションを行うコードを作成し実行してください。\n",
    "\n",
    "\n",
    "> `Cf.`\n",
    "> + [sklearn.model_selection.KFold — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
    "> + [比較的少なめのデータで機械学習する時は交差検証 (Cross Validation) をするのです - Qiita](https://qiita.com/LicaOka/items/c6725aa8961df9332cc7#%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC-cross-validation-%E3%81%A8%E3%81%AF)\n",
    "> + [Does TensorFlow have cross validation implemented for its users? - stackoverflow](https://stackoverflow.com/questions/38164798/does-tensorflow-have-cross-validation-implemented-for-its-users?rq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - CrossValidation(KFold class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t/.pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:17:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:17:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:17:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:17:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:17:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9533333333333334 \n",
      " [1.         0.93333333 0.93333333 0.93333333 0.96666667]\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "score = cross_val_score(xgb, iris_features, iris_labels, cv=cv)\n",
    "print(score.mean(), \"\\n\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題2】グリッドサーチ\n",
    ">      これまで分類器のパラメータには触れず、デフォルトの設定を使用していました。\n",
    ">      パラメータの詳細は今後のSprintで学んでいくことになります。\n",
    ">      機械学習の前提として、パラメータは状況に応じて最適なものを選ぶ必要があります。\n",
    ">      最適なパラメータを探していくことを パラメータチューニング と呼びます。\n",
    ">      パラメータチューニングをある程度自動化する単純な方法としては グリッドサーチ があります。\n",
    ">      scikit-learnのGridSearchCVを使い、グリッドサーチを行うコードを作成してください。\n",
    ">      そして、ベースラインモデルに対して何らかしらのパラメータチューニングを行なってください。\n",
    ">      どのパラメータをチューニングするかは、使用した手法の公式ドキュメントを参考にしてください。\n",
    "\n",
    "> `Cf.`\n",
    "> + [sklearn.model_selection.GridSearchCV — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    ">      GridSearchCVクラスには引数としてモデル、探索範囲、さらにクロスバリデーションを何分割で行うかを与えます。\n",
    ">      クロスバリデーションの機能も含まれているため、これを使用する場合はKFoldクラスを利用する必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題3】Kaggle Notebooksからの調査\n",
    ">      KaggleのNotebooksから様々なアイデアを見つけ出して、列挙してください。\n",
    "\n",
    "> `Cf.`\n",
    "> + [交差検証（cross validation／クロスバリデーション）の種類を整理してみた](https://aizine.ai/cross-validation0910/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題4】高い汎化性能のモデル作成\n",
    ">     問題3で見つけたアイデアと、独自のアイデアを組み合わせ高い汎化性能のモデル作りを進めてください。\n",
    ">     その過程として、何を行うことで、クロスバリデーションの結果がどの程度変化したかを表にまとめてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題5】最終的なモデルの選定\n",
    ">     最終的にこれは良いというモデルを選び、推定した結果をKaggleに提出してスコアを確認してください。\n",
    ">     どういったアイデアを取り入れ、どの程度のスコアになったかを記載してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
